{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b010542-b606-4842-992e-7fd9d2575051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Big Data Processing: Pandas and PySpark\n",
    "\n",
    "**What we'll cover:**\n",
    "1. Small data → Pandas wins\n",
    "2. Large data → PySpark required\n",
    "3. Side-by-side: Same operations in both tools\n",
    "4. Key concepts: Lazy evaluation, actions vs transformations, partitions\n",
    "5. Joins: Combining datasets\n",
    "6. Common beginner mistakes\n",
    "7. Writing results\n",
    "8. Your turn: Practice exercises\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "109c0543-63db-4fc1-b87d-ae286ad8b834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98135ebd-ba67-43ea-a44a-6adc7677f74a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark version: 4.0.0\n✅ Ready to go!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from pyspark.sql.functions import col, count, avg, sum as spark_sum, desc, when\n",
    "\n",
    "# Verify Spark is ready (spark is pre-created in Databricks)\n",
    "print(f\"✅ Spark version: {spark.version}\")\n",
    "print(f\"✅ Ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c333ae5b-3969-49b7-8c58-406a0ece8b80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Part 1: Small Data (100 MB) - Pandas is Faster\n",
    "\n",
    "**Dataset:** Flight delays  \n",
    "**Size:** ~100 MB, 1.4M rows  \n",
    "**Task:** Find top 10 airports with most delays\n",
    "\n",
    "**Hypothesis:** PySpark overhead makes it slower for small data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f548dbec-08b9-4117-81d4-65d794a36397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pandas Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1980e36-8f64-4fb6-8743-a779930c1e81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️  Pandas: 7.79 seconds\n\n\uD83D\uDCCA Top 10 airports:\norigin\nATL    41828\nORD    33812\nDEN    30760\nDFW    28706\nLAX    22684\nIAH    21009\nPHX    17555\nLAS    16938\nSFO    16552\nMCO    14189\nName: delay, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# PANDAS: Read and process\n",
    "path = \"/dbfs/databricks-datasets/flights/departuredelays.csv\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_pandas = pd.read_csv(path)\n",
    "result = df_pandas[df_pandas['delay'] > 0] \\\n",
    "    .groupby('origin')['delay'] \\\n",
    "    .count() \\\n",
    "    .sort_values(ascending=False) \\\n",
    "    .head(10)\n",
    "\n",
    "pandas_time = time.time() - start\n",
    "\n",
    "print(f\"⏱️  Pandas: {pandas_time:.2f} seconds\")\n",
    "print(f\"\\n\uD83D\uDCCA Top 10 airports:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3978038-f5d8-441e-90c4-8474dfeca386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b23aa54c-b40f-4fc5-8e0a-4190e058a994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n|origin|count|\n+------+-----+\n|   ATL|41828|\n|   ORD|33812|\n|   DEN|30760|\n|   DFW|28706|\n|   LAX|22684|\n|   IAH|21009|\n|   PHX|17555|\n|   LAS|16938|\n|   SFO|16552|\n|   MCO|14189|\n+------+-----+\n\n\n⏱️  PySpark: 2.45 seconds\n"
     ]
    }
   ],
   "source": [
    "# PYSPARK: Same task\n",
    "path = \"/databricks-datasets/flights/departuredelays.csv\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_spark = spark.read.csv(path, header=True, inferSchema=True)\n",
    "result = df_spark.filter(col('delay') > 0) \\\n",
    "    .groupBy('origin') \\\n",
    "    .count() \\\n",
    "    .orderBy(desc('count')) \\\n",
    "    .limit(10)\n",
    "\n",
    "result.show()\n",
    "\n",
    "pyspark_time = time.time() - start\n",
    "\n",
    "print(f\"\\n⏱️  PySpark: {pyspark_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e1fc8cc-68df-4afb-9d41-242bb2ca02e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\nSmall Data (100 MB)\n==================================================\nPandas:  7.79s  ← WINNER\nPySpark: 2.45s\n\nPySpark is 0.3x SLOWER\n\n\uD83D\uDCA1 Why? PySpark has 2-3 second overhead:\n   - Task scheduling\n   - Data serialization\n   - Cluster coordination\n\n   For small data, overhead > processing time!\n"
     ]
    }
   ],
   "source": [
    "# COMPARISON\n",
    "print(\"=\"*50)\n",
    "print(\"Small Data (100 MB)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Pandas:  {pandas_time:.2f}s  ← WINNER\")\n",
    "print(f\"PySpark: {pyspark_time:.2f}s\")\n",
    "print(f\"\\nPySpark is SLOWER\")\n",
    "print(\"\\n\uD83D\uDCA1 Why? PySpark has 2-3 second overhead:\")\n",
    "print(\"   - Task scheduling\")\n",
    "print(\"   - Data serialization\")\n",
    "print(\"   - Cluster coordination\")\n",
    "print(\"\\n   For small data, overhead > processing time!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40f229f9-026a-41d3-9975-d460666ac381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Part 2: Large Data (1 GB+) - PySpark Required\n",
    "\n",
    "**Dataset:** 10x duplicated flights  \n",
    "**Size:** ~1 GB, 14M rows  \n",
    "**Task:** Same analysis on bigger data\n",
    "\n",
    "**What happens:** Pandas will be slow/crash. PySpark handles it easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59187c99-c4c0-4ad2-8072-b52d794237ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCE6 Creating large dataset (10x duplication)...\n✅ Created ~1 GB dataset with 14M rows\n"
     ]
    }
   ],
   "source": [
    "# Create large dataset by duplicating\n",
    "print(\"\uD83D\uDCE6 Creating large dataset (10x duplication)...\")\n",
    "\n",
    "df_base = spark.read.csv(\"/databricks-datasets/flights/departuredelays.csv\", \n",
    "                          header=True, inferSchema=True)\n",
    "\n",
    "df_large = df_base\n",
    "for i in range(9):\n",
    "    df_large = df_large.union(df_base)\n",
    "\n",
    "print(\"✅ Created ~1 GB dataset with 14M rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38e7d39e-0067-46da-9d84-b9e93ff72544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n|origin| count|\n+------+------+\n|   ATL|418280|\n|   ORD|338120|\n|   DEN|307600|\n|   DFW|287060|\n|   LAX|226840|\n|   IAH|210090|\n|   PHX|175550|\n|   LAS|169380|\n|   SFO|165520|\n|   MCO|141890|\n+------+------+\n\n\n⏱️  PySpark on 1 GB: 2.85 seconds\n\n\uD83D\uDCA1 Pandas would:\n   - Take 3-5x longer (11.4s estimate)\n   - Or crash with MemoryError\n\n   PySpark handles it easily! ✅\n"
     ]
    }
   ],
   "source": [
    "# PYSPARK: Process large data\n",
    "start = time.time()\n",
    "\n",
    "result_large = df_large.filter(col('delay') > 0) \\\n",
    "    .groupBy('origin') \\\n",
    "    .count() \\\n",
    "    .orderBy(desc('count')) \\\n",
    "    .limit(10)\n",
    "\n",
    "result_large.show()\n",
    "\n",
    "large_time = time.time() - start\n",
    "\n",
    "print(f\"\\n⏱️  PySpark on 1 GB: {large_time:.2f} seconds\")\n",
    "print(f\"\\n\uD83D\uDCA1 Pandas would:\")\n",
    "print(f\"   - Take 3-5x longer ({large_time*4:.1f}s estimate)\")\n",
    "print(f\"   - Or crash with MemoryError\")\n",
    "print(f\"\\n   PySpark handles it easily! ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f947373-4aa6-4b65-8867-a62ffc5f8064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Part 3: Side-by-Side Comparison\n",
    "\n",
    "Let's see common operations in both tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f22b119e-2869-4fd9-bc62-52dce54586dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Operation 1: Select Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "552c9832-bb4b-48f8-9739-6427f6d2b08e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas:\n      date origin destination  delay\n0  1011245    ABE         ATL      6\n1  1020600    ABE         DTW     -8\n2  1021245    ABE         ATL     -2\n3  1020605    ABE         ATL     -4\n4  1031245    ABE         ATL     -4\n\n==================================================\n\nPySpark:\n+-------+------+-----------+-----+\n|   date|origin|destination|delay|\n+-------+------+-----------+-----+\n|1011245|   ABE|        ATL|    6|\n|1020600|   ABE|        DTW|   -8|\n|1021245|   ABE|        ATL|   -2|\n|1020605|   ABE|        ATL|   -4|\n|1031245|   ABE|        ATL|   -4|\n+-------+------+-----------+-----+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# PANDAS\n",
    "df_pandas_select = df_pandas[['date', 'origin', 'destination', 'delay']]\n",
    "print(\"Pandas:\")\n",
    "print(df_pandas_select.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# PYSPARK  \n",
    "df_spark_select = df_spark.select('date', 'origin', 'destination', 'delay')\n",
    "print(\"PySpark:\")\n",
    "df_spark_select.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c825ae4-9ef9-46d9-bad5-4fc5b4ee8c50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Operation 2: Filter Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ed2c06e-02c7-4e8b-8e05-db861a1a5c90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas: 17,218 flights with >1hr delay and >1000 miles\n\n==================================================\n\nPySpark: 17,218 flights with >1hr delay and >1000 miles\n"
     ]
    }
   ],
   "source": [
    "# PANDAS\n",
    "df_pandas_filter = df_pandas[\n",
    "    (df_pandas['delay'] > 60) & \n",
    "    (df_pandas['distance'] > 1000)\n",
    "]\n",
    "print(f\"Pandas: {len(df_pandas_filter):,} flights with >1hr delay and >1000 miles\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# PYSPARK\n",
    "df_spark_filter = df_spark.filter(\n",
    "    (col('delay') > 60) & \n",
    "    (col('distance') > 1000)\n",
    ")\n",
    "print(f\"PySpark: {df_spark_filter.count():,} flights with >1hr delay and >1000 miles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d34977d6-c49a-4f85-9116-18cdddd586dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Operation 3: Add New Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dfb4682-cd65-4e00-8487-d27250baa5b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas:\n   delay  delay_hours\n0      6     0.100000\n1     -8    -0.133333\n2     -2    -0.033333\n3     -4    -0.066667\n4     -4    -0.066667\n\n==================================================\n\nPySpark:\n+-----+--------------------+\n|delay|         delay_hours|\n+-----+--------------------+\n|    6|                 0.1|\n|   -8|-0.13333333333333333|\n|   -2|-0.03333333333333333|\n|   -4|-0.06666666666666667|\n|   -4|-0.06666666666666667|\n+-----+--------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# PANDAS: Modify dataframe directly\n",
    "df_pandas_copy = df_pandas.copy()\n",
    "df_pandas_copy['delay_hours'] = df_pandas_copy['delay'] / 60\n",
    "print(\"Pandas:\")\n",
    "print(df_pandas_copy[['delay', 'delay_hours']].head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# PYSPARK: Returns new dataframe (immutable)\n",
    "df_spark_with_hours = df_spark.withColumn('delay_hours', col('delay') / 60)\n",
    "print(\"PySpark:\")\n",
    "df_spark_with_hours.select('delay', 'delay_hours').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fdb4c0b-115f-444a-9599-bbb412842ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Operation 4: Group By & Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ecb4cee-dc4c-4215-bb32-26e72b4fd5f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas:\n        delay      \n         mean count\norigin             \nABE     11.41   448\nABI      7.26   706\nABQ     11.23  5739\nABY      6.17   252\nACT      0.90   437\n\n==================================================\n\nPySpark:\n+------+-----------------+-----+\n|origin|        avg_delay|count|\n+------+-----------------+-----+\n|   GNV|  8.4328165374677|  774|\n|   CLT|8.681008379691571|28402|\n|   DCA|8.012508036705828|17109|\n|   FAT|8.853170189098998| 2697|\n|   COD|2.374301675977654|  179|\n+------+-----------------+-----+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# PANDAS\n",
    "pandas_agg = df_pandas.groupby('origin').agg({\n",
    "    'delay': ['mean', 'count']\n",
    "}).round(2).head()\n",
    "print(\"Pandas:\")\n",
    "print(pandas_agg)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# PYSPARK\n",
    "spark_agg = df_spark.groupBy('origin').agg(\n",
    "    avg('delay').alias('avg_delay'),\n",
    "    count('*').alias('count')\n",
    ")\n",
    "print(\"PySpark:\")\n",
    "spark_agg.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "529af050-b49f-4307-8d58-b95da3ba6bd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Operation 5: Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fcb408a-f84c-4188-82a8-bec72f216b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas - Top 5 delays:\n            date origin destination  delay\n1378822  3090615    TPA         DFW   1642\n844767   2190925    SFO         ORD   1638\n620315   2021245    FLL         DFW   1636\n1309315  3020700    RSW         ORD   1592\n44799    1180805    BNA         DFW   1560\n\n==================================================\n\nPySpark - Top 5 delays:\n+-------+------+-----------+-----+\n|   date|origin|destination|delay|\n+-------+------+-----------+-----+\n|3090615|   TPA|        DFW| 1642|\n|2190925|   SFO|        ORD| 1638|\n|2021245|   FLL|        DFW| 1636|\n|3020700|   RSW|        ORD| 1592|\n|1180805|   BNA|        DFW| 1560|\n+-------+------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# PANDAS\n",
    "pandas_sorted = df_pandas.sort_values('delay', ascending=False).head(5)\n",
    "print(\"Pandas - Top 5 delays:\")\n",
    "print(pandas_sorted[['date', 'origin', 'destination', 'delay']])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# PYSPARK\n",
    "spark_sorted = df_spark.orderBy(desc('delay')).limit(5)\n",
    "print(\"PySpark - Top 5 delays:\")\n",
    "spark_sorted.select('date', 'origin', 'destination', 'delay').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dcca40e-6a02-443c-b329-47d2b1d1835d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Summary: Syntax Comparison\n",
    "\n",
    "| Operation | Pandas | PySpark |\n",
    "|-----------|--------|----------|\n",
    "| **Select** | `df[['col1', 'col2']]` | `df.select('col1', 'col2')` |\n",
    "| **Filter** | `df[df['col'] > 10]` | `df.filter(col('col') > 10)` |\n",
    "| **Add column** | `df['new'] = df['old'] * 2` | `df.withColumn('new', col('old') * 2)` |\n",
    "| **Group by** | `df.groupby('col').sum()` | `df.groupBy('col').sum()` |\n",
    "| **Sort** | `df.sort_values('col')` | `df.orderBy('col')` |\n",
    "\n",
    "\uD83D\uDCA1 **Key difference:** PySpark uses `col()` function for columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b6442b3-7345-4425-aa3c-d9643bea72b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Part 4: Key PySpark Concepts\n",
    "\n",
    "Understanding these concepts is critical for working effectively with PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c247ece8-09e6-41fd-9372-46151b3938b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Concept 1: Lazy Evaluation\n",
    "\n",
    "**Pandas:** Executes immediately  \n",
    "**PySpark:** Builds a plan, executes when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "318aacea-dfd2-4f2d-b8c3-794d5042e195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ PySpark Lazy Evaluation Demo\n\nBuilding plan: 0.0003s (basically instant!)\nExecuting: 2.79s (now it actually runs)\n\nResult: 1,897,660 flights\n\n\uD83D\uDCA1 All work happened in .count(), not before!\n"
     ]
    }
   ],
   "source": [
    "print(\"⚡ PySpark Lazy Evaluation Demo\\n\")\n",
    "\n",
    "# These operations are INSTANT (just building a plan)\n",
    "start = time.time()\n",
    "\n",
    "df_lazy = df_large.filter(col('delay') > 0)\n",
    "df_lazy = df_lazy.select('origin', 'delay')\n",
    "df_lazy = df_lazy.filter(col('delay') > 30)\n",
    "\n",
    "build_time = time.time() - start\n",
    "print(f\"Building plan: {build_time:.4f}s (basically instant!)\")\n",
    "\n",
    "# THIS triggers execution\n",
    "start = time.time()\n",
    "count = df_lazy.count()  # ← ACTION\n",
    "exec_time = time.time() - start\n",
    "\n",
    "print(f\"Executing: {exec_time:.2f}s (now it actually runs)\")\n",
    "print(f\"\\nResult: {count:,} flights\")\n",
    "print(f\"\\n\uD83D\uDCA1 All work happened in .count(), not before!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c78132aa-857b-4296-90a2-61e00788746d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Concept 2: Actions vs Transformations (CRITICAL!)\n",
    "\n",
    "This is the most important concept in PySpark!\n",
    "\n",
    "**Transformations** (lazy - just build the plan):\n",
    "- `.filter()`, `.select()`, `.groupBy()`, `.join()`, `.withColumn()`\n",
    "- Don't execute anything\n",
    "- Return a new DataFrame\n",
    "\n",
    "**Actions** (trigger execution):\n",
    "- `.count()`, `.show()`, `.collect()`, `.write()`\n",
    "- Actually run the computation\n",
    "- Return results to driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "907c7605-4717-4b27-91db-2c6f266a6a94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD0D Actions vs Transformations Demo\n\n1. Building query with transformations...\n   Time: 0.0004s (instant!)\n\n2. Triggering with .count() action...\n   Time: 2.91s (now it runs!)\n   Result: 78 airports\n\n\uD83D\uDCA1 Key takeaway:\n   - Chain transformations freely (they're free!)\n   - Minimize actions (they're expensive!)\n   - Don't call .count() unnecessarily!\n"
     ]
    }
   ],
   "source": [
    "print(\"\uD83D\uDD0D Actions vs Transformations Demo\\n\")\n",
    "\n",
    "# TRANSFORMATIONS - instant\n",
    "print(\"1. Building query with transformations...\")\n",
    "start = time.time()\n",
    "\n",
    "query = df_large \\\n",
    "    .filter(col('delay') > 60) \\\n",
    "    .filter(col('distance') > 1000) \\\n",
    "    .groupBy('origin') \\\n",
    "    .count() \\\n",
    "    .filter(col('count') > 100)\n",
    "\n",
    "transform_time = time.time() - start\n",
    "print(f\"   Time: {transform_time:.4f}s (instant!)\\n\")\n",
    "\n",
    "# ACTION - triggers everything\n",
    "print(\"2. Triggering with .count() action...\")\n",
    "start = time.time()\n",
    "result = query.count()\n",
    "action_time = time.time() - start\n",
    "\n",
    "print(f\"   Time: {action_time:.2f}s (now it runs!)\")\n",
    "print(f\"   Result: {result} airports\\n\")\n",
    "\n",
    "print(\"\uD83D\uDCA1 Key takeaway:\")\n",
    "print(\"   - Chain transformations freely (they're free!)\")\n",
    "print(\"   - Minimize actions (they're expensive!)\")\n",
    "print(\"   - Don't call .count() unnecessarily!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8ec1f81-e975-4cf5-a95e-7bdc6b6d37d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Quick Reference: Actions vs Transformations\n",
    "\n",
    "| Type | Operations | What They Do |\n",
    "|------|-----------|-------------|\n",
    "| **Transformations** | `.filter()`, `.select()`, `.groupBy()`, `.join()`, `.withColumn()`, `.orderBy()` | Build execution plan (lazy) |\n",
    "| **Actions** | `.count()`, `.show()`, `.collect()`, `.take()`, `.first()`, `.write()` | Trigger execution |\n",
    "\n",
    "**Rule of thumb:** If it returns a DataFrame, it's a transformation. If it returns a value or writes data, it's an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "299fd22f-40a6-4d44-837c-fc71fc4716c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Concept 3: Partitions - How Spark Parallelizes\n",
    "\n",
    "Data is split into **partitions** - think of them as chunks that can be processed in parallel.\n",
    "\n",
    "- More partitions = more parallelism = faster (usually)\n",
    "- But too many = overhead from coordination\n",
    "- Rule of thumb: 2-4 partitions per CPU core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6944d33-e8ad-430e-a8b7-a8e7a8facbab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCE6 Partitions Demo\n\n\uD83D\uDCA1 Partitions split your data into chunks for parallel processing\n   - Databricks serverless automatically manages partitions\n   - You can still manually repartition if needed\n\nTesting different partition strategies...\n\n✓ Fewer partitions (4): 3.50s\n✓ More partitions (32): 3.39s\n✓ Default (auto): 2.52s\n\n\uD83D\uDCA1 Key takeaways:\n   - More partitions ≠ always faster\n   - Too few = not enough parallelism\n   - Too many = coordination overhead\n   - Usually best to let Spark decide!\n\n\uD83D\uDCDD Note: On standard clusters, you can check partition count with:\n   df.rdd.getNumPartitions() (not available on serverless)\n"
     ]
    }
   ],
   "source": [
    "print(\"\uD83D\uDCE6 Partitions Demo\\n\")\n",
    "print(\"\uD83D\uDCA1 Partitions split your data into chunks for parallel processing\")\n",
    "print(\"   - Databricks serverless automatically manages partitions\")\n",
    "print(\"   - You can still manually repartition if needed\\n\")\n",
    "\n",
    "# Repartition to different sizes\n",
    "print(\"Testing different partition strategies...\\n\")\n",
    "\n",
    "# Fewer partitions (4)\n",
    "df_few = df_large.repartition(4)\n",
    "start = time.time()\n",
    "df_few.filter(col('delay') > 0).count()\n",
    "few_time = time.time() - start\n",
    "print(f\"✓ Fewer partitions (4): {few_time:.2f}s\")\n",
    "\n",
    "# More partitions (32)\n",
    "df_many = df_large.repartition(32)\n",
    "start = time.time()\n",
    "df_many.filter(col('delay') > 0).count()\n",
    "many_time = time.time() - start\n",
    "print(f\"✓ More partitions (32): {many_time:.2f}s\")\n",
    "\n",
    "# Default (let Spark decide)\n",
    "start = time.time()\n",
    "df_large.filter(col('delay') > 0).count()\n",
    "default_time = time.time() - start\n",
    "print(f\"✓ Default (auto): {default_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDCA1 Key takeaways:\")\n",
    "print(f\"   - More partitions ≠ always faster\")\n",
    "print(f\"   - Too few = not enough parallelism\")\n",
    "print(f\"   - Too many = coordination overhead\")\n",
    "print(f\"   - Usually best to let Spark decide!\")\n",
    "print(f\"\\n\uD83D\uDCDD Note: On standard clusters, you can check partition count with:\")\n",
    "print(f\"   df.rdd.getNumPartitions() (not available on serverless)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae14b529-a7ce-4b1e-a755-4a429923fd50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Concept 4: Filter Early = Faster\n",
    "\n",
    "Always filter data BEFORE expensive operations like groupBy or join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ea3881b-4d87-4d73-9e24-7d7a6c0cc232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83C\uDFAF Optimization: Filter Early\n\n❌ Bad:  3.36s (grouped all data)\n✅ Good: 3.10s (filtered first)\n\n⚡ 1.1x speedup by filtering early!\n"
     ]
    }
   ],
   "source": [
    "print(\"\uD83C\uDFAF Optimization: Filter Early\\n\")\n",
    "\n",
    "# ❌ BAD: Group all 14M rows first\n",
    "start = time.time()\n",
    "bad = df_large.groupBy('origin').count().filter(col('count') > 1000).count()\n",
    "bad_time = time.time() - start\n",
    "print(f\"❌ Bad:  {bad_time:.2f}s (grouped all data)\")\n",
    "\n",
    "# ✅ GOOD: Filter to 7M rows first\n",
    "start = time.time()\n",
    "good = df_large.filter(col('delay') > 0).groupBy('origin').count().filter(col('count') > 1000).count()\n",
    "good_time = time.time() - start\n",
    "print(f\"✅ Good: {good_time:.2f}s (filtered first)\")\n",
    "\n",
    "print(f\"\\n⚡ {bad_time/good_time:.1f}x speedup by filtering early!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7671a7bd-abd7-40d8-bde8-a4bfa13c8db9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Part 5: Joins - Combining DataFrames\n",
    "\n",
    "Joins are one of the most common operations in real-world data work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ba00a1b-8c6b-44fe-9dc6-a2095de11099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Sample Airport Information Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af980742-421d-4035-a4a8-775d99ecada9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airport Information:\n+----+-------------+----------+\n|code|         city|     state|\n+----+-------------+----------+\n| SFO|San Francisco|California|\n| LAX|  Los Angeles|California|\n| ORD|      Chicago|  Illinois|\n| ATL|      Atlanta|   Georgia|\n| DFW|       Dallas|     Texas|\n| JFK|     New York|  New York|\n| SEA|      Seattle|Washington|\n| DEN|       Denver|  Colorado|\n+----+-------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create a small airport info dataset\n",
    "airport_data = [\n",
    "    ('SFO', 'San Francisco', 'California'),\n",
    "    ('LAX', 'Los Angeles', 'California'),\n",
    "    ('ORD', 'Chicago', 'Illinois'),\n",
    "    ('ATL', 'Atlanta', 'Georgia'),\n",
    "    ('DFW', 'Dallas', 'Texas'),\n",
    "    ('JFK', 'New York', 'New York'),\n",
    "    ('SEA', 'Seattle', 'Washington'),\n",
    "    ('DEN', 'Denver', 'Colorado')\n",
    "]\n",
    "\n",
    "airport_df = spark.createDataFrame(airport_data, ['code', 'city', 'state'])\n",
    "print(\"Airport Information:\")\n",
    "airport_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6cb9176-9d14-4cc1-976e-95378cfe088c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Inner Join - Get Delays with Airport Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62382eae-4ca1-449b-aea1-f8ab283ca6c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD17 INNER JOIN Example\n\nTop delayed flights with city names:\n+------+-------------+----------+-----------+-----+\n|origin|  origin_city|     state|destination|delay|\n+------+-------------+----------+-----------+-----+\n|   SFO|San Francisco|California|        ORD| 1638|\n|   DEN|       Denver|  Colorado|        DFW| 1174|\n|   ORD|      Chicago|  Illinois|        MIA| 1149|\n|   JFK|     New York|  New York|        MCO| 1014|\n|   JFK|     New York|  New York|        DEN| 1003|\n|   DEN|       Denver|  Colorado|        MSP|  982|\n|   LAX|  Los Angeles|California|        ATL|  926|\n|   ATL|      Atlanta|   Georgia|        PHL|  925|\n|   JFK|     New York|  New York|        HNL|  922|\n|   JFK|     New York|  New York|        SEA|  920|\n+------+-------------+----------+-----------+-----+\nonly showing top 10 rows\n\n\uD83D\uDCA1 Inner join only keeps rows that match in BOTH DataFrames\n"
     ]
    }
   ],
   "source": [
    "print(\"\uD83D\uDD17 INNER JOIN Example\\n\")\n",
    "\n",
    "# Get top delayed flights with city names\n",
    "delayed_flights = df_spark.filter(col('delay') > 60).select('origin', 'delay', 'destination')\n",
    "\n",
    "# Join to get origin city name\n",
    "result = delayed_flights.join(\n",
    "    airport_df,\n",
    "    delayed_flights.origin == airport_df.code,\n",
    "    'inner'  # only keep matches\n",
    ").select(\n",
    "    col('origin'),\n",
    "    col('city').alias('origin_city'),\n",
    "    col('state'),\n",
    "    col('destination'),\n",
    "    col('delay')\n",
    ").orderBy(desc('delay'))\n",
    "\n",
    "print(\"Top delayed flights with city names:\")\n",
    "result.show(10)\n",
    "\n",
    "print(\"\\n\uD83D\uDCA1 Inner join only keeps rows that match in BOTH DataFrames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cad6d3d-71ba-41d5-82a1-cba6afeef084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Left Join - Keep All Flights, Add Info When Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19fd4480-5cf2-4834-86d4-c95ac923667a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD17 LEFT JOIN Example\n\nAll flights (with city name when available):\n+------+-----------+-----------+-----+\n|origin|origin_city|destination|delay|\n+------+-----------+-----------+-----+\n|   ABE|       NULL|        ATL|   88|\n|   ABE|       NULL|        ATL|   69|\n|   ABE|       NULL|        DTW|  151|\n|   ABE|       NULL|        ORD|   83|\n|   ABE|       NULL|        ATL|  127|\n|   ABE|       NULL|        ORD|   68|\n|   ABE|       NULL|        DTW|   89|\n|   ABE|       NULL|        DTW|   80|\n|   ABE|       NULL|        ATL|  333|\n|   ABE|       NULL|        ORD|  219|\n+------+-----------+-----------+-----+\nonly showing top 10 rows\n\n\uD83D\uDCA1 Left join kept all flights: 63,918 have no city info (NULL)\n"
     ]
    }
   ],
   "source": [
    "print(\"\uD83D\uDD17 LEFT JOIN Example\\n\")\n",
    "\n",
    "# Left join keeps all flights, adds city info when available\n",
    "result_left = delayed_flights.join(\n",
    "    airport_df,\n",
    "    delayed_flights.origin == airport_df.code,\n",
    "    'left'  # keep all from left (delayed_flights)\n",
    ").select(\n",
    "    col('origin'),\n",
    "    col('city').alias('origin_city'),\n",
    "    col('destination'),\n",
    "    col('delay')\n",
    ")\n",
    "\n",
    "print(\"All flights (with city name when available):\")\n",
    "result_left.show(10)\n",
    "\n",
    "# Count nulls (airports not in our lookup table)\n",
    "null_count = result_left.filter(col('origin_city').isNull()).count()\n",
    "print(f\"\\n\uD83D\uDCA1 Left join kept all flights: {null_count:,} have no city info (NULL)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7d5320e-85f6-40f8-a0cd-9ab68ce4d537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Join Types Quick Reference\n",
    "\n",
    "| Join Type | Keeps | Use When |\n",
    "|-----------|-------|----------|\n",
    "| **inner** | Only matches | Need complete info for both sides |\n",
    "| **left** | All from left + matches from right | Keep all primary data, enrich with lookup |\n",
    "| **right** | All from right + matches from left | Rarely used (just flip to left) |\n",
    "| **outer** | Everything from both | Need to see all data, matched or not |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47fd3904-293f-4416-84e0-99b153c04d83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Part 6: Common Beginner Mistakes (AVOID THESE!)\n",
    "\n",
    "These mistakes will crash your code or make it super slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9e869c0-614c-4b58-b244-bc3af90fd732",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Mistake 1: Using .collect() on Large Data\n",
    "\n",
    "**THE CARDINAL SIN OF PYSPARK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a3e1726-4652-4e3d-9dfb-910d7baa57d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISTAKE #1: .collect() on large data\n\n❌ BAD: df_large.collect()\n   → Brings ALL data to driver memory\n   → WILL CRASH with OutOfMemory error\n   → Use .show() or .limit() instead\n\n✅ GOOD: Peek at data\n+-------+-----+--------+------+-----------+\n|   date|delay|distance|origin|destination|\n+-------+-----+--------+------+-----------+\n|1011245|    6|     602|   ABE|        ATL|\n|1020600|   -8|     369|   ABE|        DTW|\n|1021245|   -2|     602|   ABE|        ATL|\n|1020605|   -4|     602|   ABE|        ATL|\n|1031245|   -4|     602|   ABE|        ATL|\n|1030605|    0|     602|   ABE|        ATL|\n|1041243|   10|     602|   ABE|        ATL|\n|1040605|   28|     602|   ABE|        ATL|\n|1051245|   88|     602|   ABE|        ATL|\n|1050605|    9|     602|   ABE|        ATL|\n|1061215|   -6|     602|   ABE|        ATL|\n|1061725|   69|     602|   ABE|        ATL|\n|1061230|    0|     369|   ABE|        DTW|\n|1060625|   -3|     602|   ABE|        ATL|\n|1070600|    0|     369|   ABE|        DTW|\n|1071725|    0|     602|   ABE|        ATL|\n|1071230|    0|     369|   ABE|        DTW|\n|1070625|    0|     602|   ABE|        ATL|\n|1071219|    0|     569|   ABE|        ORD|\n|1080600|    0|     369|   ABE|        DTW|\n+-------+-----+--------+------+-----------+\nonly showing top 20 rows\n\n✅ GOOD: Sample for local analysis\nSample size: 1000 rows\n\n\uD83D\uDCA1 RULE: Never .collect() unless you're sure data fits in memory!\n"
     ]
    }
   ],
   "source": [
    "print(\"MISTAKE #1: .collect() on large data\\n\")\n",
    "\n",
    "# ❌ NEVER DO THIS on large data\n",
    "print(\"❌ BAD: df_large.collect()\")\n",
    "print(\"   → Brings ALL data to driver memory\")\n",
    "print(\"   → WILL CRASH with OutOfMemory error\")\n",
    "print(\"   → Use .show() or .limit() instead\\n\")\n",
    "\n",
    "# ✅ GOOD alternatives\n",
    "print(\"✅ GOOD: Peek at data\")\n",
    "df_large.show(20)  # Safe - only shows 20 rows\n",
    "\n",
    "print(\"\\n✅ GOOD: Sample for local analysis\")\n",
    "sample = df_large.limit(1000).toPandas()  # Safe - limited to 1000 rows\n",
    "print(f\"Sample size: {len(sample)} rows\")\n",
    "\n",
    "print(\"\\n\uD83D\uDCA1 RULE: Never .collect() unless you're sure data fits in memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4464a26-c3c0-4d3a-87a7-a5350ebbeea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Mistake 2: Excessive .count() Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "162ead60-0339-48a8-a331-49f1889272a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ MISTAKE #2: Too many .count() calls\n\n❌ BAD: Counting at every step\n   After filter: 5,917,270\n   After filter 2: 3,455,930\n   Time: 5.15s\n\n✅ GOOD: Count once at the end\n   Final count: 3,455,930\n   Time: 2.66s\n\n⚡ 1.9x faster!\n\n\uD83D\uDCA1 RULE: .count() is expensive. Use it sparingly!\n"
     ]
    }
   ],
   "source": [
    "print(\"⚠️ MISTAKE #2: Too many .count() calls\\n\")\n",
    "\n",
    "# ❌ BAD: Multiple counts\n",
    "print(\"❌ BAD: Counting at every step\")\n",
    "start = time.time()\n",
    "\n",
    "step1 = df_large.filter(col('delay') > 0)\n",
    "print(f\"   After filter: {step1.count():,}\")  # Expensive!\n",
    "\n",
    "step2 = step1.filter(col('distance') > 500)\n",
    "print(f\"   After filter 2: {step2.count():,}\")  # Expensive!\n",
    "\n",
    "bad_time = time.time() - start\n",
    "print(f\"   Time: {bad_time:.2f}s\\n\")\n",
    "\n",
    "# ✅ GOOD: Count only at the end\n",
    "print(\"✅ GOOD: Count once at the end\")\n",
    "start = time.time()\n",
    "\n",
    "result = df_large \\\n",
    "    .filter(col('delay') > 0) \\\n",
    "    .filter(col('distance') > 500)\n",
    "\n",
    "final_count = result.count()\n",
    "good_time = time.time() - start\n",
    "\n",
    "print(f\"   Final count: {final_count:,}\")\n",
    "print(f\"   Time: {good_time:.2f}s\")\n",
    "print(f\"\\n⚡ {bad_time/good_time:.1f}x faster!\\n\")\n",
    "\n",
    "print(\"\uD83D\uDCA1 RULE: .count() is expensive. Use it sparingly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59b65fdd-3bec-4872-abf8-5b6cb5a096c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Mistake 3: Not Viewing Execution Plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "533d0f4a-a744-49b0-ae26-65e0ebdc94f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD0D Use .explain() to see what Spark is doing\n\nExecution plan:\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   ColumnarToRow\n   +- PhotonResultStage\n      +- PhotonSort [count#14938L DESC NULLS LAST]\n         +- PhotonShuffleExchangeSource\n            +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#23338]\n               +- PhotonShuffleExchangeSink rangepartitioning(count#14938L DESC NULLS LAST, 1024)\n                  +- PhotonGroupingAgg(keys=[origin#13380], functions=[finalmerge_count(merge count#14986L) AS count(1)#14984L])\n                     +- PhotonShuffleExchangeSource\n                        +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#23332]\n                           +- PhotonShuffleExchangeSink hashpartitioning(origin#13380, 1024)\n                              +- PhotonGroupingAgg(keys=[origin#13380], functions=[partial_count(1) AS count#14986L])\n                                 +- PhotonUnion Generic\n                                    :- PhotonProject [origin#13380]\n                                    :  +- PhotonFilter (isnotnull(delay#13378) AND (delay#13378 > 60))\n                                    :     +- PhotonRowToColumnar\n                                    :        +- FileScan csv [delay#13378,origin#13380] Batched: false, DataFilters: [isnotnull(delay#13378), (delay#13378 > 60)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/flights/departuredelays.csv], PartitionFilters: [], PushedFilters: [IsNotNull(delay), GreaterThan(delay,60)], ReadSchema: struct<delay:int,origin:string>\n                                    :- PhotonProject [origin#14942]\n                                    :  +- PhotonFilter (isnotnull(delay#14940) AND (delay#14940 > 60))\n                                    :     +- PhotonRowToColumnar\n                                    :        +- FileScan csv [delay#14940,origin#14942] Batched: false, DataFilters: [isnotnull(delay#14940), (delay#14940 > 60)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/flights/departuredelays.csv], PartitionFilters: [], PushedFilters: [IsNotNull(delay), GreaterThan(delay,60)], ReadSchema: struct<delay:int,origin:string>\n                                    :- PhotonProject [origin#14947]\n                                    :  +- PhotonFilter (isnotnull(delay#14945) AND (delay#14945 > 60))\n                                    :     +- PhotonRowToColumnar\n                                    :        +- FileScan csv [delay#14945,origin#14947] Batched: false, DataFilters: [isnotnull(delay#14945), (delay#14945 > 60)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/flights/departuredelays.csv], PartitionFilters: [], PushedFilters: [IsNotNull(delay), GreaterThan(delay,60)], ReadSchema: struct<delay:int,origin:string>\n                                    :- PhotonProject [origin#14952]\n                                    :  +- PhotonFilter (isnotnull(delay#14950) AND (delay#14950 > 60))\n                                    :     +- PhotonRowToColumnar\n                                    :        +- FileScan csv [delay#14950,origin#14952] Batched: false, DataFilters: [isnotnull(delay#14950), (delay#14950 > 60)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/flights/departuredelays.csv], PartitionFilters: [], PushedFilters: [IsNotNull(delay), GreaterThan(delay,60)], ReadSchema: struct<delay:int,origin:string>\n                                    :- PhotonProject [origin#14957]\n                                    :  +- PhotonFilter (isnotnull(delay#14955) AND (delay#14955 > 60))\n                                    :     +- PhotonRowToColumnar\n                                    :        +- FileScan csv [delay#14955,origin#14957] Batched: false, DataFilters: [isnotnull(delay#14955), (delay#14955 > 60)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/flights/departuredelays.csv], PartitionFilters: [], PushedFilters: [IsNotNull(delay), GreaterThan(delay,60)], ReadSchema: struct<delay:int,origin:string>\n                                    :- PhotonProject [origin#14962]\n                                    :  +- PhotonFilter (isnotnull(delay#14960) AND (delay#14960 > 60))\n                                    :     +- PhotonRowToColumnar\n                                    :        +- FileScan csv [delay#14960,origin#14962] Batched: false, DataFilters: [isnotnull(delay#14960), (delay#14960 > 60)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/flights/departuredelays.csv], PartitionFilters: [], PushedFilters: [IsNotNull(delay), GreaterThan(delay,60)], ReadSchema: struct<delay:int,origin:string>\n                                    :- PhotonProject [origin#14967]\n                                    :  +- PhotonFilter (isnotnull(delay#14965) AND (delay#14965 > 60))\n                                    :     +- PhotonRowToColumnar\n                                    :        +- FileScan csv [delay#14965,origin#14967] Batched: false, DataFilters: [isnotnull(delay#14965), (delay#14965 > 60)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/flights/departuredelays.csv], PartitionFilters: [], PushedFilters: [IsNotNull(delay), GreaterThan(delay,60)], ReadSchema: struct<delay:int,origin:string>\n                                    :- PhotonProject [origin#14972]\n                                    :  +- PhotonFilter (isnotnull(delay#14970) AND (delay#14970 > 60))\n                                    :     +- PhotonRowToColumnar\n                                    :        +- FileScan csv [delay#14970,origin#14972] Batched: false, DataFilters: [isnotnull(delay#14970), (delay#14970 > 60)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/flights/departuredelays.csv], PartitionFilters: [], PushedFilters: [IsNotNull(delay), GreaterThan(delay,60)], ReadSchema: struct<delay:int,origin:string>\n                                    :- PhotonProject [origin#14977]\n                                    :  +- PhotonFilter (isnotnull(delay#14975) AND (delay#14975 > 60))\n                                    :     +- PhotonRowToColumnar\n                                    :        +- FileScan csv [delay#14975,origin#14977] Batched: false, DataFilters: [isnotnull(delay#14975), (delay#14975 > 60)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/flights/departuredelays.csv], PartitionFilters: [], PushedFilters: [IsNotNull(delay), GreaterThan(delay,60)], ReadSchema: struct<delay:int,origin:string>\n                                    +- PhotonProject [origin#14982]\n                                       +- PhotonFilter (isnotnull(delay#14980) AND (delay#14980 > 60))\n                                          +- PhotonRowToColumnar\n                                             +- FileScan csv [delay#14980,origin#14982] Batched: false, DataFilters: [isnotnull(delay#14980), (delay#14980 > 60)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/flights/departuredelays.csv], PartitionFilters: [], PushedFilters: [IsNotNull(delay), GreaterThan(delay,60)], ReadSchema: struct<delay:int,origin:string>\n\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n\n\uD83D\uDCA1 .explain() helps you understand:\n   - What operations Spark will do\n   - In what order\n   - Where optimizations happen\n   - Useful for debugging slow queries!\n"
     ]
    }
   ],
   "source": [
    "print(\"\uD83D\uDD0D Use .explain() to see what Spark is doing\\n\")\n",
    "\n",
    "query = df_large \\\n",
    "    .filter(col('delay') > 60) \\\n",
    "    .groupBy('origin') \\\n",
    "    .count() \\\n",
    "    .orderBy(desc('count'))\n",
    "\n",
    "# See the execution plan\n",
    "print(\"Execution plan:\")\n",
    "query.explain()\n",
    "\n",
    "print(\"\\n\uD83D\uDCA1 .explain() helps you understand:\")\n",
    "print(\"   - What operations Spark will do\")\n",
    "print(\"   - In what order\")\n",
    "print(\"   - Where optimizations happen\")\n",
    "print(\"   - Useful for debugging slow queries!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26150622-2181-445a-a20a-2454abdadd21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Mistake 4: Forgetting to Filter Before Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40674643-b5ce-41be-8331-07a06e0644ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83C\uDFAF MISTAKE #4: Not filtering before joins\n\n❌ BAD: Join everything first\n   Time: 2.86s\n\n✅ GOOD: Filter before join\n   Time: 2.98s\n\n⚡ 1.0x faster!\n\n\uD83D\uDCA1 RULE: Always filter before expensive operations like joins!\n"
     ]
    }
   ],
   "source": [
    "print(\"\uD83C\uDFAF MISTAKE #4: Not filtering before joins\\n\")\n",
    "\n",
    "# ❌ BAD: Join all data\n",
    "print(\"❌ BAD: Join everything first\")\n",
    "start = time.time()\n",
    "bad_result = df_large \\\n",
    "    .join(airport_df, df_large.origin == airport_df.code, 'inner') \\\n",
    "    .filter(col('delay') > 100) \\\n",
    "    .count()\n",
    "bad_time = time.time() - start\n",
    "print(f\"   Time: {bad_time:.2f}s\\n\")\n",
    "\n",
    "# ✅ GOOD: Filter first\n",
    "print(\"✅ GOOD: Filter before join\")\n",
    "start = time.time()\n",
    "good_result = df_large \\\n",
    "    .filter(col('delay') > 100) \\\n",
    "    .join(airport_df, df_large.origin == airport_df.code, 'inner') \\\n",
    "    .count()\n",
    "good_time = time.time() - start\n",
    "print(f\"   Time: {good_time:.2f}s\")\n",
    "print(f\"\\n⚡ {bad_time/good_time:.1f}x faster!\\n\")\n",
    "\n",
    "print(\"\uD83D\uDCA1 RULE: Always filter before expensive operations like joins!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f0acfdd-498e-4c12-88e9-28dfa5bd7471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Common Mistakes Summary\n",
    "\n",
    "| ❌ Don't Do This | ✅ Do This Instead |\n",
    "|-----------------|-------------------|\n",
    "| `.collect()` on large data | `.show()` or `.limit().toPandas()` |\n",
    "| `.count()` after every step | `.count()` only when needed |\n",
    "| Never check execution plan | Use `.explain()` to understand |\n",
    "| Join then filter | Filter then join |\n",
    "| Large `.show(10000)` | `.show(20)` is usually enough |\n",
    "| Ignore errors | Read error messages (they're helpful!) |\n",
    "\n",
    "**Remember: PySpark is lazy. Take advantage of it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07f8de2b-a572-4d72-858c-97fe46740329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Part 7: Writing Results\n",
    "\n",
    "You've processed your data - now save it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc272673-794f-4956-ad72-8970a4be79d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write to Parquet (Recommended for big data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feb7d99c-0db1-499f-a69c-693ac2db0f50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCBE Writing Results\n\nPreview of what would be saved: 939,850 rows\n+-------+------+-----------+-----+--------+\n|   date|origin|destination|delay|distance|\n+-------+------+-----------+-----+--------+\n|1051245|   ABE|        ATL|   88|     602|\n|1061725|   ABE|        ATL|   69|     602|\n|1090600|   ABE|        DTW|  151|     369|\n|1091219|   ABE|        ORD|   83|     569|\n|1111215|   ABE|        ATL|  127|     602|\n+-------+------+-----------+-----+--------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"\uD83D\uDCBE Writing Results\\n\")\n",
    "\n",
    "# Create a result to save\n",
    "result_to_save = df_large \\\n",
    "    .filter(col('delay') > 60) \\\n",
    "    .select('date', 'origin', 'destination', 'delay', 'distance')\n",
    "    \n",
    "print(f\"Preview of what would be saved: {result_to_save.count():,} rows\")\n",
    "result_to_save.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b84e3480-aaae-4014-b6fc-bccdd61cd4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write to CSV (For small results or Excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8124958-abc8-447b-a6dd-5b10aadeea37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>origin</th><th>avg_delay</th><th>flight_count</th></tr></thead><tbody><tr><td>GUM</td><td>302.0</td><td>90</td></tr><tr><td>ALO</td><td>222.23076923076923</td><td>130</td></tr><tr><td>YUM</td><td>191.77777777777777</td><td>180</td></tr><tr><td>BGR</td><td>191.25</td><td>160</td></tr><tr><td>CDV</td><td>176.85714285714286</td><td>70</td></tr><tr><td>MOT</td><td>176.76</td><td>250</td></tr><tr><td>EGE</td><td>175.71428571428572</td><td>980</td></tr><tr><td>ADQ</td><td>174.4</td><td>50</td></tr><tr><td>ACT</td><td>172.5</td><td>100</td></tr><tr><td>HNL</td><td>170.21596244131456</td><td>2130</td></tr><tr><td>AVP</td><td>169.16666666666666</td><td>180</td></tr><tr><td>ROA</td><td>168.09615384615384</td><td>520</td></tr><tr><td>PIA</td><td>165.33333333333334</td><td>630</td></tr><tr><td>PIH</td><td>164.85714285714286</td><td>70</td></tr><tr><td>FAY</td><td>161.85714285714286</td><td>420</td></tr><tr><td>YAK</td><td>161.36363636363637</td><td>110</td></tr><tr><td>ORF</td><td>159.4228855721393</td><td>2010</td></tr><tr><td>AEX</td><td>159.29032258064515</td><td>620</td></tr><tr><td>VPS</td><td>159.06410256410257</td><td>780</td></tr><tr><td>GUC</td><td>156.95652173913044</td><td>230</td></tr><tr><td>JAN</td><td>156.33333333333334</td><td>1020</td></tr><tr><td>FCA</td><td>156.0</td><td>210</td></tr><tr><td>OME</td><td>155.28571428571428</td><td>70</td></tr><tr><td>BTV</td><td>154.02173913043478</td><td>920</td></tr><tr><td>SYR</td><td>153.31159420289856</td><td>1380</td></tr><tr><td>LSE</td><td>152.73684210526315</td><td>190</td></tr><tr><td>SWF</td><td>152.52631578947367</td><td>190</td></tr><tr><td>ABI</td><td>151.90243902439025</td><td>410</td></tr><tr><td>ABE</td><td>151.71428571428572</td><td>350</td></tr><tr><td>SGF</td><td>151.42758620689656</td><td>1450</td></tr><tr><td>SRQ</td><td>151.38</td><td>1000</td></tr><tr><td>GSO</td><td>151.26282051282053</td><td>1560</td></tr><tr><td>TXK</td><td>151.14285714285714</td><td>70</td></tr><tr><td>TYR</td><td>151.07407407407408</td><td>270</td></tr><tr><td>RIC</td><td>150.82417582417582</td><td>2730</td></tr><tr><td>BIL</td><td>150.75</td><td>320</td></tr><tr><td>MSO</td><td>150.525</td><td>400</td></tr><tr><td>CID</td><td>150.32743362831857</td><td>1130</td></tr><tr><td>SIT</td><td>150.08333333333334</td><td>240</td></tr><tr><td>KOA</td><td>149.6851851851852</td><td>540</td></tr><tr><td>SJT</td><td>148.6315789473684</td><td>190</td></tr><tr><td>SJU</td><td>146.79591836734693</td><td>5390</td></tr><tr><td>LCH</td><td>146.6969696969697</td><td>330</td></tr><tr><td>CMI</td><td>146.46428571428572</td><td>560</td></tr><tr><td>HPN</td><td>146.42384105960264</td><td>1510</td></tr><tr><td>OGG</td><td>146.0097087378641</td><td>1030</td></tr><tr><td>EVV</td><td>145.9245283018868</td><td>530</td></tr><tr><td>HRL</td><td>145.52631578947367</td><td>570</td></tr><tr><td>JFK</td><td>145.36513934813416</td><td>21170</td></tr><tr><td>BPT</td><td>144.92307692307693</td><td>130</td></tr><tr><td>GTF</td><td>144.63636363636363</td><td>220</td></tr><tr><td>MQT</td><td>144.5</td><td>100</td></tr><tr><td>BUF</td><td>143.16722408026754</td><td>2990</td></tr><tr><td>RDD</td><td>143.08</td><td>250</td></tr><tr><td>ALB</td><td>142.85470085470087</td><td>1170</td></tr><tr><td>CAK</td><td>142.76041666666666</td><td>960</td></tr><tr><td>JNU</td><td>142.27906976744185</td><td>430</td></tr><tr><td>COS</td><td>142.07692307692307</td><td>1820</td></tr><tr><td>FSD</td><td>142.03246753246754</td><td>1540</td></tr><tr><td>CLL</td><td>141.5</td><td>260</td></tr><tr><td>RDM</td><td>141.38095238095238</td><td>420</td></tr><tr><td>CVG</td><td>140.88050314465409</td><td>3180</td></tr><tr><td>BQN</td><td>140.65384615384616</td><td>260</td></tr><tr><td>TYS</td><td>140.42352941176472</td><td>1700</td></tr><tr><td>LEX</td><td>140.2736842105263</td><td>950</td></tr><tr><td>RSW</td><td>140.25714285714287</td><td>6650</td></tr><tr><td>CAE</td><td>140.05389221556885</td><td>1670</td></tr><tr><td>MOB</td><td>140.05376344086022</td><td>930</td></tr><tr><td>SPS</td><td>140.0</td><td>140</td></tr><tr><td>LAN</td><td>139.90322580645162</td><td>310</td></tr><tr><td>MDT</td><td>139.54651162790697</td><td>860</td></tr><tr><td>MHT</td><td>139.17241379310346</td><td>1160</td></tr><tr><td>BMI</td><td>138.74</td><td>500</td></tr><tr><td>PBI</td><td>138.69070512820514</td><td>6240</td></tr><tr><td>ROC</td><td>138.28877005347593</td><td>1870</td></tr><tr><td>PWM</td><td>138.14634146341464</td><td>820</td></tr><tr><td>BDL</td><td>138.04</td><td>3000</td></tr><tr><td>LNK</td><td>137.90322580645162</td><td>620</td></tr><tr><td>XNA</td><td>137.37142857142857</td><td>2100</td></tr><tr><td>OKC</td><td>137.30434782608697</td><td>3220</td></tr><tr><td>SCE</td><td>137.11111111111111</td><td>180</td></tr><tr><td>HDN</td><td>137.03333333333333</td><td>300</td></tr><tr><td>GRR</td><td>136.75</td><td>2360</td></tr><tr><td>MFR</td><td>136.7037037037037</td><td>810</td></tr><tr><td>JAX</td><td>136.68767908309457</td><td>3490</td></tr><tr><td>CLE</td><td>136.62327909887358</td><td>7990</td></tr><tr><td>LRD</td><td>136.54054054054055</td><td>370</td></tr><tr><td>BTR</td><td>136.47682119205297</td><td>1510</td></tr><tr><td>PVD</td><td>136.36046511627907</td><td>1720</td></tr><tr><td>FNT</td><td>136.25</td><td>560</td></tr><tr><td>DAB</td><td>135.92857142857142</td><td>280</td></tr><tr><td>MKE</td><td>135.87179487179486</td><td>5850</td></tr><tr><td>SDF</td><td>135.7004048582996</td><td>2470</td></tr><tr><td>IAD</td><td>135.6847290640394</td><td>14210</td></tr><tr><td>CHA</td><td>135.62264150943398</td><td>530</td></tr><tr><td>MSN</td><td>135.5</td><td>1560</td></tr><tr><td>OAJ</td><td>135.4516129032258</td><td>310</td></tr><tr><td>FAR</td><td>135.16949152542372</td><td>1180</td></tr><tr><td>DAY</td><td>134.48170731707316</td><td>1640</td></tr><tr><td>MLB</td><td>134.47058823529412</td><td>170</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "GUM",
         302.0,
         90
        ],
        [
         "ALO",
         222.23076923076923,
         130
        ],
        [
         "YUM",
         191.77777777777777,
         180
        ],
        [
         "BGR",
         191.25,
         160
        ],
        [
         "CDV",
         176.85714285714286,
         70
        ],
        [
         "MOT",
         176.76,
         250
        ],
        [
         "EGE",
         175.71428571428572,
         980
        ],
        [
         "ADQ",
         174.4,
         50
        ],
        [
         "ACT",
         172.5,
         100
        ],
        [
         "HNL",
         170.21596244131456,
         2130
        ],
        [
         "AVP",
         169.16666666666666,
         180
        ],
        [
         "ROA",
         168.09615384615384,
         520
        ],
        [
         "PIA",
         165.33333333333334,
         630
        ],
        [
         "PIH",
         164.85714285714286,
         70
        ],
        [
         "FAY",
         161.85714285714286,
         420
        ],
        [
         "YAK",
         161.36363636363637,
         110
        ],
        [
         "ORF",
         159.4228855721393,
         2010
        ],
        [
         "AEX",
         159.29032258064515,
         620
        ],
        [
         "VPS",
         159.06410256410257,
         780
        ],
        [
         "GUC",
         156.95652173913044,
         230
        ],
        [
         "JAN",
         156.33333333333334,
         1020
        ],
        [
         "FCA",
         156.0,
         210
        ],
        [
         "OME",
         155.28571428571428,
         70
        ],
        [
         "BTV",
         154.02173913043478,
         920
        ],
        [
         "SYR",
         153.31159420289856,
         1380
        ],
        [
         "LSE",
         152.73684210526315,
         190
        ],
        [
         "SWF",
         152.52631578947367,
         190
        ],
        [
         "ABI",
         151.90243902439025,
         410
        ],
        [
         "ABE",
         151.71428571428572,
         350
        ],
        [
         "SGF",
         151.42758620689656,
         1450
        ],
        [
         "SRQ",
         151.38,
         1000
        ],
        [
         "GSO",
         151.26282051282053,
         1560
        ],
        [
         "TXK",
         151.14285714285714,
         70
        ],
        [
         "TYR",
         151.07407407407408,
         270
        ],
        [
         "RIC",
         150.82417582417582,
         2730
        ],
        [
         "BIL",
         150.75,
         320
        ],
        [
         "MSO",
         150.525,
         400
        ],
        [
         "CID",
         150.32743362831857,
         1130
        ],
        [
         "SIT",
         150.08333333333334,
         240
        ],
        [
         "KOA",
         149.6851851851852,
         540
        ],
        [
         "SJT",
         148.6315789473684,
         190
        ],
        [
         "SJU",
         146.79591836734693,
         5390
        ],
        [
         "LCH",
         146.6969696969697,
         330
        ],
        [
         "CMI",
         146.46428571428572,
         560
        ],
        [
         "HPN",
         146.42384105960264,
         1510
        ],
        [
         "OGG",
         146.0097087378641,
         1030
        ],
        [
         "EVV",
         145.9245283018868,
         530
        ],
        [
         "HRL",
         145.52631578947367,
         570
        ],
        [
         "JFK",
         145.36513934813416,
         21170
        ],
        [
         "BPT",
         144.92307692307693,
         130
        ],
        [
         "GTF",
         144.63636363636363,
         220
        ],
        [
         "MQT",
         144.5,
         100
        ],
        [
         "BUF",
         143.16722408026754,
         2990
        ],
        [
         "RDD",
         143.08,
         250
        ],
        [
         "ALB",
         142.85470085470087,
         1170
        ],
        [
         "CAK",
         142.76041666666666,
         960
        ],
        [
         "JNU",
         142.27906976744185,
         430
        ],
        [
         "COS",
         142.07692307692307,
         1820
        ],
        [
         "FSD",
         142.03246753246754,
         1540
        ],
        [
         "CLL",
         141.5,
         260
        ],
        [
         "RDM",
         141.38095238095238,
         420
        ],
        [
         "CVG",
         140.88050314465409,
         3180
        ],
        [
         "BQN",
         140.65384615384616,
         260
        ],
        [
         "TYS",
         140.42352941176472,
         1700
        ],
        [
         "LEX",
         140.2736842105263,
         950
        ],
        [
         "RSW",
         140.25714285714287,
         6650
        ],
        [
         "CAE",
         140.05389221556885,
         1670
        ],
        [
         "MOB",
         140.05376344086022,
         930
        ],
        [
         "SPS",
         140.0,
         140
        ],
        [
         "LAN",
         139.90322580645162,
         310
        ],
        [
         "MDT",
         139.54651162790697,
         860
        ],
        [
         "MHT",
         139.17241379310346,
         1160
        ],
        [
         "BMI",
         138.74,
         500
        ],
        [
         "PBI",
         138.69070512820514,
         6240
        ],
        [
         "ROC",
         138.28877005347593,
         1870
        ],
        [
         "PWM",
         138.14634146341464,
         820
        ],
        [
         "BDL",
         138.04,
         3000
        ],
        [
         "LNK",
         137.90322580645162,
         620
        ],
        [
         "XNA",
         137.37142857142857,
         2100
        ],
        [
         "OKC",
         137.30434782608697,
         3220
        ],
        [
         "SCE",
         137.11111111111111,
         180
        ],
        [
         "HDN",
         137.03333333333333,
         300
        ],
        [
         "GRR",
         136.75,
         2360
        ],
        [
         "MFR",
         136.7037037037037,
         810
        ],
        [
         "JAX",
         136.68767908309457,
         3490
        ],
        [
         "CLE",
         136.62327909887358,
         7990
        ],
        [
         "LRD",
         136.54054054054055,
         370
        ],
        [
         "BTR",
         136.47682119205297,
         1510
        ],
        [
         "PVD",
         136.36046511627907,
         1720
        ],
        [
         "FNT",
         136.25,
         560
        ],
        [
         "DAB",
         135.92857142857142,
         280
        ],
        [
         "MKE",
         135.87179487179486,
         5850
        ],
        [
         "SDF",
         135.7004048582996,
         2470
        ],
        [
         "IAD",
         135.6847290640394,
         14210
        ],
        [
         "CHA",
         135.62264150943398,
         530
        ],
        [
         "MSN",
         135.5,
         1560
        ],
        [
         "OAJ",
         135.4516129032258,
         310
        ],
        [
         "FAR",
         135.16949152542372,
         1180
        ],
        [
         "DAY",
         134.48170731707316,
         1640
        ],
        [
         "MLB",
         134.47058823529412,
         170
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "origin",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "avg_delay",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "flight_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "small_result = (\n",
    "    df_large\n",
    "    .filter(\n",
    "        col('delay') > 60\n",
    "    )\n",
    "    .groupBy('origin')\n",
    "    .agg(\n",
    "        avg('delay').alias('avg_delay'),\n",
    "        count('*').alias('flight_count')\n",
    "    )\n",
    "    .orderBy(\n",
    "        desc('avg_delay')\n",
    "    )\n",
    "    .limit(100)\n",
    ")\n",
    "\n",
    "display(small_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b123201-6efc-4375-bb04-76bf96c75c45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write Modes\n",
    "\n",
    "| Mode | What It Does | Use When |\n",
    "|------|-------------|----------|\n",
    "| `overwrite` | Replace existing data | Recreating results |\n",
    "| `append` | Add to existing data | Incremental updates |\n",
    "| `ignore` | Skip if exists | Don't want to overwrite |\n",
    "| `error` | Fail if exists | Safety - don't overwrite accidentally |\n",
    "\n",
    "**Most common:** `overwrite` for one-off analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8faf4af-d1ec-4eb6-8491-5cb7ee947f83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### File Formats Quick Reference\n",
    "\n",
    "| Format | Best For | Pros | Cons |\n",
    "|--------|----------|------|------|\n",
    "| **Parquet** | Big data storage | Fast, compressed, columnar | Not human-readable |\n",
    "| **CSV** | Small results, Excel | Readable, universal | Slow, no compression |\n",
    "| **JSON** | Nested data, APIs | Flexible schema | Larger files |\n",
    "| **Delta** | Production data lakes | ACID, time travel, fast | Needs Delta Lake setup |\n",
    "\n",
    "**Default choice: Use Parquet for everything unless you have a reason not to.**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "workshop_complete",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}