{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b010542-b606-4842-992e-7fd9d2575051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Big Data Processing: Pandas and PySpark\n",
    "\n",
    "**What we'll cover:**\n",
    "1. Small data → Pandas wins\n",
    "2. Large data → PySpark required\n",
    "3. Side-by-side: Same operations in both tools\n",
    "4. Key concepts: Lazy evaluation, actions vs transformations, partitions\n",
    "5. Joins: Combining datasets\n",
    "6. Common beginner mistakes\n",
    "7. Writing results\n",
    "8. Your turn: Practice exercises\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "109c0543-63db-4fc1-b87d-ae286ad8b834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98135ebd-ba67-43ea-a44a-6adc7677f74a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from pyspark.sql.functions import col, count, avg, sum as spark_sum, desc, when\n",
    "\n",
    "# Verify Spark is ready (spark is pre-created in Databricks)\n",
    "print(f\"✅ Spark version: {spark.version}\")\n",
    "print(f\"✅ Ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c333ae5b-3969-49b7-8c58-406a0ece8b80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Part 1: Small Data (100 MB) - Pandas is Faster\n",
    "\n",
    "**Dataset:** Flight delays  \n",
    "**Size:** ~100 MB, 1.4M rows  \n",
    "**Task:** Find top 10 airports with most delays\n",
    "\n",
    "**Hypothesis:** PySpark overhead makes it slower for small data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f548dbec-08b9-4117-81d4-65d794a36397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pandas Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1980e36-8f64-4fb6-8743-a779930c1e81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PANDAS: Read and process\n",
    "path = \"/dbfs/databricks-datasets/flights/departuredelays.csv\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_pandas = pd.read_csv(path)\n",
    "result = df_pandas[df_pandas['delay'] > 0] \\\n",
    "    .groupby('origin')['delay'] \\\n",
    "    .count() \\\n",
    "    .sort_values(ascending=False) \\\n",
    "    .head(10)\n",
    "\n",
    "pandas_time = time.time() - start\n",
    "\n",
    "print(f\"⏱️  Pandas: {pandas_time:.2f} seconds\")\n",
    "print(f\"\\n\uD83D\uDCCA Top 10 airports:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3978038-f5d8-441e-90c4-8474dfeca386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b23aa54c-b40f-4fc5-8e0a-4190e058a994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PYSPARK: Same task\n",
    "path = \"/databricks-datasets/flights/departuredelays.csv\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_spark = spark.read.csv(path, header=True, inferSchema=True)\n",
    "result = df_spark.filter(col('delay') > 0) \\\n",
    "    .groupBy('origin') \\\n",
    "    .count() \\\n",
    "    .orderBy(desc('count')) \\\n",
    "    .limit(10)\n",
    "\n",
    "result.show()\n",
    "\n",
    "pyspark_time = time.time() - start\n",
    "\n",
    "print(f\"\\n  PySpark: {pyspark_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e1fc8cc-68df-4afb-9d41-242bb2ca02e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMPARISON\n",
    "print(\"=\"*50)\n",
    "print(\"Small Data (100 MB)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Pandas:  {pandas_time:.2f}s  ← WINNER\")\n",
    "print(f\"PySpark: {pyspark_time:.2f}s\")\n",
    "print(f\"\\nPySpark is SLOWER\")\n",
    "print(\"\\n\uD83D\uDCA1 Why? PySpark has 2-3 second overhead:\")\n",
    "print(\"   - Task scheduling\")\n",
    "print(\"   - Data serialization\")\n",
    "print(\"   - Cluster coordination\")\n",
    "print(\"\\n   For small data, overhead > processing time!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40f229f9-026a-41d3-9975-d460666ac381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Part 2: Large Data (1 GB+) - PySpark Required\n",
    "\n",
    "**Dataset:** 10x duplicated flights  \n",
    "**Size:** ~1 GB, 14M rows  \n",
    "**Task:** Same analysis on bigger data\n",
    "\n",
    "**What happens:** Pandas will be slow/crash. PySpark handles it easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59187c99-c4c0-4ad2-8072-b52d794237ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create large dataset by duplicating\n",
    "print(\"Creating large dataset (10x duplication)...\")\n",
    "\n",
    "df_base = spark.read.csv(\"/databricks-datasets/flights/departuredelays.csv\", \n",
    "                          header=True, inferSchema=True)\n",
    "\n",
    "df_large = df_base\n",
    "for i in range(9):\n",
    "    df_large = df_large.union(df_base)\n",
    "\n",
    "print(\"Created ~1 GB dataset with 14M rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38e7d39e-0067-46da-9d84-b9e93ff72544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PYSPARK: Process large data\n",
    "start = time.time()\n",
    "\n",
    "result_large = df_large.filter(col('delay') > 0) \\\n",
    "    .groupBy('origin') \\\n",
    "    .count() \\\n",
    "    .orderBy(desc('count')) \\\n",
    "    .limit(10)\n",
    "\n",
    "result_large.show()\n",
    "\n",
    "large_time = time.time() - start\n",
    "\n",
    "print(f\"\\n⏱️  PySpark on 1 GB: {large_time:.2f} seconds\")\n",
    "print(f\"\\n\uD83D\uDCA1 Pandas would:\")\n",
    "print(f\"   - Take 3-5x longer ({large_time*4:.1f}s estimate)\")\n",
    "print(f\"   - Or crash with MemoryError\")\n",
    "print(f\"\\n   PySpark handles it easily! ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f947373-4aa6-4b65-8867-a62ffc5f8064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Part 3: Side-by-Side Comparison\n",
    "\n",
    "Let's see common operations in both tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f22b119e-2869-4fd9-bc62-52dce54586dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Operation 1: Select Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "552c9832-bb4b-48f8-9739-6427f6d2b08e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PANDAS\n",
    "df_pandas_select = df_pandas[['date', 'origin', 'destination', 'delay']]\n",
    "print(\"Pandas:\")\n",
    "print(df_pandas_select.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# PYSPARK  \n",
    "df_spark_select = df_spark.select('date', 'origin', 'destination', 'delay')\n",
    "print(\"PySpark:\")\n",
    "df_spark_select.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c825ae4-9ef9-46d9-bad5-4fc5b4ee8c50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Operation 2: Filter Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ed2c06e-02c7-4e8b-8e05-db861a1a5c90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PANDAS\n",
    "df_pandas_filter = df_pandas[\n",
    "    (df_pandas['delay'] > 60) & \n",
    "    (df_pandas['distance'] > 1000)\n",
    "]\n",
    "print(f\"Pandas: {len(df_pandas_filter):,} flights with >1hr delay and >1000 miles\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# PYSPARK\n",
    "df_spark_filter = df_spark.filter(\n",
    "    (col('delay') > 60) & \n",
    "    (col('distance') > 1000)\n",
    ")\n",
    "print(f\"PySpark: {df_spark_filter.count():,} flights with >1hr delay and >1000 miles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d34977d6-c49a-4f85-9116-18cdddd586dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Operation 3: Add New Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dfb4682-cd65-4e00-8487-d27250baa5b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PANDAS: Modify dataframe directly\n",
    "df_pandas_copy = df_pandas.copy()\n",
    "df_pandas_copy['delay_hours'] = df_pandas_copy['delay'] / 60\n",
    "print(\"Pandas:\")\n",
    "print(df_pandas_copy[['delay', 'delay_hours']].head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# PYSPARK: Returns new dataframe (immutable)\n",
    "df_spark_with_hours = df_spark.withColumn('delay_hours', col('delay') / 60)\n",
    "print(\"PySpark:\")\n",
    "df_spark_with_hours.select('delay', 'delay_hours').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fdb4c0b-115f-444a-9599-bbb412842ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Operation 4: Group By & Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ecb4cee-dc4c-4215-bb32-26e72b4fd5f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PANDAS\n",
    "pandas_agg = df_pandas.groupby('origin').agg({\n",
    "    'delay': ['mean', 'count']\n",
    "}).round(2).head()\n",
    "print(\"Pandas:\")\n",
    "print(pandas_agg)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# PYSPARK\n",
    "spark_agg = df_spark.groupBy('origin').agg(\n",
    "    avg('delay').alias('avg_delay'),\n",
    "    count('*').alias('count')\n",
    ")\n",
    "print(\"PySpark:\")\n",
    "spark_agg.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "529af050-b49f-4307-8d58-b95da3ba6bd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Operation 5: Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fcb408a-f84c-4188-82a8-bec72f216b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PANDAS\n",
    "pandas_sorted = df_pandas.sort_values('delay', ascending=False).head(5)\n",
    "print(\"Pandas - Top 5 delays:\")\n",
    "print(pandas_sorted[['date', 'origin', 'destination', 'delay']])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# PYSPARK\n",
    "spark_sorted = df_spark.orderBy(desc('delay')).limit(5)\n",
    "print(\"PySpark - Top 5 delays:\")\n",
    "spark_sorted.select('date', 'origin', 'destination', 'delay').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dcca40e-6a02-443c-b329-47d2b1d1835d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Summary: Syntax Comparison\n",
    "\n",
    "| Operation | Pandas | PySpark |\n",
    "|-----------|--------|----------|\n",
    "| **Select** | `df[['col1', 'col2']]` | `df.select('col1', 'col2')` |\n",
    "| **Filter** | `df[df['col'] > 10]` | `df.filter(col('col') > 10)` |\n",
    "| **Add column** | `df['new'] = df['old'] * 2` | `df.withColumn('new', col('old') * 2)` |\n",
    "| **Group by** | `df.groupby('col').sum()` | `df.groupBy('col').sum()` |\n",
    "| **Sort** | `df.sort_values('col')` | `df.orderBy('col')` |\n",
    "\n",
    "\uD83D\uDCA1 **Key difference:** PySpark uses `col()` function for columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b6442b3-7345-4425-aa3c-d9643bea72b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Part 4: Key PySpark Concepts\n",
    "\n",
    "Understanding these concepts is critical for working effectively with PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c247ece8-09e6-41fd-9372-46151b3938b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Concept 1: Lazy Evaluation\n",
    "\n",
    "**Pandas:** Executes immediately  \n",
    "**PySpark:** Builds a plan, executes when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "318aacea-dfd2-4f2d-b8c3-794d5042e195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Lazy Evaluation Example \n",
    "\n",
    "# Intentionally inefficient order:\n",
    "# 1. Create column for ALL rows (expensive)\n",
    "# 2. Filter afterwards (wasteful)\n",
    "\n",
    "df = spark.read.csv(\"/databricks-datasets/flights/departuredelays.csv\", \n",
    "                    header=True, inferSchema=True)\n",
    "\n",
    "query = (df\n",
    "    .select('origin', 'destination', 'delay', 'distance')\n",
    "    .withColumn('delay_category',\n",
    "                when(col('delay') > 100, 'severe')\n",
    "                .when(col('delay') > 30, 'moderate')\n",
    "                .otherwise('minor'))\n",
    "    .filter(col('delay') > 60)           # Written AFTER withColumn\n",
    "    .filter(col('distance') > 500)       \n",
    "    .groupBy('origin')\n",
    "    .agg(avg('delay').alias('avg_delay'), count('*').alias('count'))\n",
    "    .orderBy(desc('avg_delay'))\n",
    ")\n",
    "\n",
    "display(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c78132aa-857b-4296-90a2-61e00788746d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Concept 2: Actions vs Transformations (CRITICAL!)\n",
    "\n",
    "This is the most important concept in PySpark!\n",
    "\n",
    "**Transformations** (lazy - just build the plan):\n",
    "- `.filter()`, `.select()`, `.groupBy()`, `.join()`, `.withColumn()`\n",
    "- Don't execute anything\n",
    "- Return a new DataFrame\n",
    "\n",
    "**Actions** (trigger execution):\n",
    "- `.count()`, `.show()`, `.collect()`, `.write()`\n",
    "- Actually run the computation\n",
    "- Return results to driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "907c7605-4717-4b27-91db-2c6f266a6a94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Actions vs Transformations\n",
    "\n",
    "import time\n",
    "\n",
    "df = spark.read.csv(\"/databricks-datasets/flights/departuredelays.csv\", \n",
    "                    header=True, inferSchema=True)\n",
    "\n",
    "# Transformations - lazy (just build a plan)\n",
    "start = time.time()\n",
    "filtered = df.filter(col('delay') > 100)\n",
    "selected = filtered.select('origin', 'delay', 'distance')\n",
    "print(f\"Transformations: {time.time() - start:.4f}s\")\n",
    "\n",
    "# Actions - eager (trigger execution)\n",
    "print(\"\\nAction 1:\")\n",
    "start = time.time()\n",
    "count = selected.count()\n",
    "print(f\"count() = {count} rows, took {time.time() - start:.2f}s\")\n",
    "\n",
    "print(\"\\nAction 2:\")\n",
    "start = time.time()\n",
    "selected.show(5)\n",
    "print(f\"show() took {time.time() - start:.2f}s\")\n",
    "\n",
    "print(\"\\nNotice: Each action re-executes the transformations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8ec1f81-e975-4cf5-a95e-7bdc6b6d37d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Quick Reference: Actions vs Transformations\n",
    "\n",
    "| Type | Operations | What They Do |\n",
    "|------|-----------|-------------|\n",
    "| **Transformations** | `.filter()`, `.select()`, `.groupBy()`, `.join()`, `.withColumn()`, `.orderBy()` | Build execution plan (lazy) |\n",
    "| **Actions** | `.count()`, `.show()`, `.collect()`, `.take()`, `.first()`, `.write()` | Trigger execution |\n",
    "\n",
    "**Rule of thumb:** If it returns a DataFrame, it's a transformation. If it returns a value or writes data, it's an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "299fd22f-40a6-4d44-837c-fc71fc4716c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Concept 3: Partitions - How Spark Parallelizes\n",
    "\n",
    "Data is split into **partitions** - think of them as chunks that can be processed in parallel.\n",
    "\n",
    "- More partitions = more parallelism = faster (usually)\n",
    "- But too many = overhead from coordination\n",
    "- Rule of thumb: 2-4 partitions per CPU core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6944d33-e8ad-430e-a8b7-a8e7a8facbab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\uD83D\uDCE6 Partitions Demo\\n\")\n",
    "print(\"\uD83D\uDCA1 Partitions split your data into chunks for parallel processing\")\n",
    "print(\"   - Databricks serverless automatically manages partitions\")\n",
    "print(\"   - You can still manually repartition if needed\\n\")\n",
    "\n",
    "# Repartition to different sizes\n",
    "print(\"Testing different partition strategies...\\n\")\n",
    "\n",
    "# Fewer partitions (4)\n",
    "df_few = df_large.repartition(4)\n",
    "start = time.time()\n",
    "df_few.filter(col('delay') > 0).count()\n",
    "few_time = time.time() - start\n",
    "print(f\"✓ Fewer partitions (4): {few_time:.2f}s\")\n",
    "\n",
    "# More partitions (32)\n",
    "df_many = df_large.repartition(32)\n",
    "start = time.time()\n",
    "df_many.filter(col('delay') > 0).count()\n",
    "many_time = time.time() - start\n",
    "print(f\"✓ More partitions (32): {many_time:.2f}s\")\n",
    "\n",
    "# Default (let Spark decide)\n",
    "start = time.time()\n",
    "df_large.filter(col('delay') > 0).count()\n",
    "default_time = time.time() - start\n",
    "print(f\"✓ Default (auto): {default_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDCA1 Key takeaways:\")\n",
    "print(f\"   - More partitions ≠ always faster\")\n",
    "print(f\"   - Too few = not enough parallelism\")\n",
    "print(f\"   - Too many = coordination overhead\")\n",
    "print(f\"   - Usually best to let Spark decide!\")\n",
    "print(f\"\\n\uD83D\uDCDD Note: On standard clusters, you can check partition count with:\")\n",
    "print(f\"   df.rdd.getNumPartitions() (not available on serverless)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae14b529-a7ce-4b1e-a755-4a429923fd50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Concept 4: Filter Early = Faster\n",
    "\n",
    "Always filter data BEFORE expensive operations like groupBy or join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ea3881b-4d87-4d73-9e24-7d7a6c0cc232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\uD83C\uDFAF Optimization: Filter Early\\n\")\n",
    "\n",
    "# BAD: Group all 14M rows first\n",
    "start = time.time()\n",
    "bad = df_large.groupBy('origin').count().filter(col('count') > 1000).count()\n",
    "bad_time = time.time() - start\n",
    "print(f\"Bad:  {bad_time:.2f}s (grouped all data)\")\n",
    "\n",
    "# GOOD: Filter to 7M rows first\n",
    "start = time.time()\n",
    "good = df_large.filter(col('delay') > 0).groupBy('origin').count().filter(col('count') > 1000).count()\n",
    "good_time = time.time() - start\n",
    "print(f\"Good: {good_time:.2f}s (filtered first)\")\n",
    "\n",
    "print(f\"\\n⚡ {bad_time/good_time:.1f}x speedup by filtering early!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7671a7bd-abd7-40d8-bde8-a4bfa13c8db9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Part 5: Joins - Combining DataFrames\n",
    "\n",
    "Joins are one of the most common operations in real-world data work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ba00a1b-8c6b-44fe-9dc6-a2095de11099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Sample Airport Information Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af980742-421d-4035-a4a8-775d99ecada9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a small airport info dataset\n",
    "airport_data = [\n",
    "    ('SFO', 'San Francisco', 'California'),\n",
    "    ('LAX', 'Los Angeles', 'California'),\n",
    "    ('ORD', 'Chicago', 'Illinois'),\n",
    "    ('ATL', 'Atlanta', 'Georgia'),\n",
    "    ('DFW', 'Dallas', 'Texas'),\n",
    "    ('JFK', 'New York', 'New York'),\n",
    "    ('SEA', 'Seattle', 'Washington'),\n",
    "    ('DEN', 'Denver', 'Colorado')\n",
    "]\n",
    "\n",
    "airport_df = spark.createDataFrame(airport_data, ['code', 'city', 'state'])\n",
    "print(\"Airport Information:\")\n",
    "airport_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6cb9176-9d14-4cc1-976e-95378cfe088c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Inner Join - Get Delays with Airport Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62382eae-4ca1-449b-aea1-f8ab283ca6c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\uD83D\uDD17 INNER JOIN Example\\n\")\n",
    "\n",
    "# Get top delayed flights with city names\n",
    "delayed_flights = df_spark.filter(col('delay') > 60).select('origin', 'delay', 'destination')\n",
    "\n",
    "# Join to get origin city name\n",
    "result = delayed_flights.join(\n",
    "    airport_df,\n",
    "    delayed_flights.origin == airport_df.code,\n",
    "    'inner'  # only keep matches\n",
    ").select(\n",
    "    col('origin'),\n",
    "    col('city').alias('origin_city'),\n",
    "    col('state'),\n",
    "    col('destination'),\n",
    "    col('delay')\n",
    ").orderBy(desc('delay'))\n",
    "\n",
    "print(\"Top delayed flights with city names:\")\n",
    "result.show(10)\n",
    "\n",
    "print(\"\\n\uD83D\uDCA1 Inner join only keeps rows that match in BOTH DataFrames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cad6d3d-71ba-41d5-82a1-cba6afeef084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Left Join - Keep All Flights, Add Info When Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19fd4480-5cf2-4834-86d4-c95ac923667a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\uD83D\uDD17 LEFT JOIN Example\\n\")\n",
    "\n",
    "# Left join keeps all flights, adds city info when available\n",
    "result_left = delayed_flights.join(\n",
    "    airport_df,\n",
    "    delayed_flights.origin == airport_df.code,\n",
    "    'left'  # keep all from left (delayed_flights)\n",
    ").select(\n",
    "    col('origin'),\n",
    "    col('city').alias('origin_city'),\n",
    "    col('destination'),\n",
    "    col('delay')\n",
    ")\n",
    "\n",
    "print(\"All flights (with city name when available):\")\n",
    "result_left.show(10)\n",
    "\n",
    "# Count nulls (airports not in our lookup table)\n",
    "null_count = result_left.filter(col('origin_city').isNull()).count()\n",
    "print(f\"\\n\uD83D\uDCA1 Left join kept all flights: {null_count:,} have no city info (NULL)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7d5320e-85f6-40f8-a0cd-9ab68ce4d537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Join Types Quick Reference\n",
    "\n",
    "| Join Type | Keeps | Use When |\n",
    "|-----------|-------|----------|\n",
    "| **inner** | Only matches | Need complete info for both sides |\n",
    "| **left** | All from left + matches from right | Keep all primary data, enrich with lookup |\n",
    "| **right** | All from right + matches from left | Rarely used (just flip to left) |\n",
    "| **outer** | Everything from both | Need to see all data, matched or not |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47fd3904-293f-4416-84e0-99b153c04d83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Part 6: Common Beginner Mistakes (AVOID THESE!)\n",
    "\n",
    "These mistakes will crash your code or make it super slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9e869c0-614c-4b58-b244-bc3af90fd732",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Mistake 1: Using .collect() on Large Data\n",
    "\n",
    "**THE CARDINAL SIN OF PYSPARK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a3e1726-4652-4e3d-9dfb-910d7baa57d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"MISTAKE #1: .collect() on large data\\n\")\n",
    "\n",
    "# ❌ NEVER DO THIS on large data\n",
    "print(\"❌ BAD: df_large.collect()\")\n",
    "print(\"   → Brings ALL data to driver memory\")\n",
    "print(\"   → WILL CRASH with OutOfMemory error\")\n",
    "print(\"   → Use .show() or .limit() instead\\n\")\n",
    "\n",
    "# ✅ GOOD alternatives\n",
    "print(\"✅ GOOD: Peek at data\")\n",
    "df_large.show(20)  # Safe - only shows 20 rows\n",
    "\n",
    "print(\"\\n✅ GOOD: Sample for local analysis\")\n",
    "sample = df_large.limit(1000).toPandas()  # Safe - limited to 1000 rows\n",
    "print(f\"Sample size: {len(sample)} rows\")\n",
    "\n",
    "print(\"\\n\uD83D\uDCA1 RULE: Never .collect() unless you're sure data fits in memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4464a26-c3c0-4d3a-87a7-a5350ebbeea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Mistake 2: Excessive .count() Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "162ead60-0339-48a8-a331-49f1889272a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"⚠️ MISTAKE #2: Too many .count() calls\\n\")\n",
    "\n",
    "# ❌ BAD: Multiple counts\n",
    "print(\"❌ BAD: Counting at every step\")\n",
    "start = time.time()\n",
    "\n",
    "step1 = df_large.filter(col('delay') > 0)\n",
    "print(f\"   After filter: {step1.count():,}\")  # Expensive!\n",
    "\n",
    "step2 = step1.filter(col('distance') > 500)\n",
    "print(f\"   After filter 2: {step2.count():,}\")  # Expensive!\n",
    "\n",
    "bad_time = time.time() - start\n",
    "print(f\"   Time: {bad_time:.2f}s\\n\")\n",
    "\n",
    "# ✅ GOOD: Count only at the end\n",
    "print(\"✅ GOOD: Count once at the end\")\n",
    "start = time.time()\n",
    "\n",
    "result = df_large \\\n",
    "    .filter(col('delay') > 0) \\\n",
    "    .filter(col('distance') > 500)\n",
    "\n",
    "final_count = result.count()\n",
    "good_time = time.time() - start\n",
    "\n",
    "print(f\"   Final count: {final_count:,}\")\n",
    "print(f\"   Time: {good_time:.2f}s\")\n",
    "print(f\"\\n⚡ {bad_time/good_time:.1f}x faster!\\n\")\n",
    "\n",
    "print(\"\uD83D\uDCA1 RULE: .count() is expensive. Use it sparingly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59b65fdd-3bec-4872-abf8-5b6cb5a096c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Mistake 3: Not Viewing Execution Plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "533d0f4a-a744-49b0-ae26-65e0ebdc94f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\uD83D\uDD0D Use .explain() to see what Spark is doing\\n\")\n",
    "\n",
    "query = df_large \\\n",
    "    .filter(col('delay') > 60) \\\n",
    "    .groupBy('origin') \\\n",
    "    .count() \\\n",
    "    .orderBy(desc('count'))\n",
    "\n",
    "# See the execution plan\n",
    "print(\"Execution plan:\")\n",
    "query.explain()\n",
    "\n",
    "print(\"\\n\uD83D\uDCA1 .explain() helps you understand:\")\n",
    "print(\"   - What operations Spark will do\")\n",
    "print(\"   - In what order\")\n",
    "print(\"   - Where optimizations happen\")\n",
    "print(\"   - Useful for debugging slow queries!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26150622-2181-445a-a20a-2454abdadd21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Mistake 4: Forgetting to Filter Before Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40674643-b5ce-41be-8331-07a06e0644ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\uD83C\uDFAF MISTAKE #4: Not filtering before joins\\n\")\n",
    "\n",
    "# ❌ BAD: Join all data\n",
    "print(\"❌ BAD: Join everything first\")\n",
    "start = time.time()\n",
    "bad_result = df_large \\\n",
    "    .join(airport_df, df_large.origin == airport_df.code, 'inner') \\\n",
    "    .filter(col('delay') > 100) \\\n",
    "    .count()\n",
    "bad_time = time.time() - start\n",
    "print(f\"   Time: {bad_time:.2f}s\\n\")\n",
    "\n",
    "# ✅ GOOD: Filter first\n",
    "print(\"✅ GOOD: Filter before join\")\n",
    "start = time.time()\n",
    "good_result = df_large \\\n",
    "    .filter(col('delay') > 100) \\\n",
    "    .join(airport_df, df_large.origin == airport_df.code, 'inner') \\\n",
    "    .count()\n",
    "good_time = time.time() - start\n",
    "print(f\"   Time: {good_time:.2f}s\")\n",
    "print(f\"\\n⚡ {bad_time/good_time:.1f}x faster!\\n\")\n",
    "\n",
    "print(\"\uD83D\uDCA1 RULE: Always filter before expensive operations like joins!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f0acfdd-498e-4c12-88e9-28dfa5bd7471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Common Mistakes Summary\n",
    "\n",
    "| ❌ Don't Do This | ✅ Do This Instead |\n",
    "|-----------------|-------------------|\n",
    "| `.collect()` on large data | `.show()` or `.limit().toPandas()` |\n",
    "| `.count()` after every step | `.count()` only when needed |\n",
    "| Never check execution plan | Use `.explain()` to understand |\n",
    "| Join then filter | Filter then join |\n",
    "| Large `.show(10000)` | `.show(20)` is usually enough |\n",
    "| Ignore errors | Read error messages (they're helpful!) |\n",
    "\n",
    "**Remember: PySpark is lazy. Take advantage of it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "431752b9-0e92-4376-af8c-721a25908941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7ba4147-6e28-4357-be25-f9227b8c89cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 7: Running SQL Queries in PySpark\n",
    "\n",
    "PySpark allows you to use SQL syntax alongside DataFrame operations. This is useful if you're more comfortable with SQL or need to work with existing SQL queries.\n",
    "\n",
    "**Key steps:**\n",
    "1. **Register a DataFrame as a temporary view** using `.createOrReplaceTempView()`\n",
    "2. **Run SQL queries** using `spark.sql()`\n",
    "3. **Mix and match** - You can combine SQL queries with DataFrame operations\n",
    "\n",
    "The result of `spark.sql()` is a DataFrame, so you can chain additional transformations or actions on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a5a311-1a29-42e8-8c4b-8548ad50fe14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Running SQL queries in PySpark\n",
    "\n",
    "df = spark.read.csv(\"/databricks-datasets/flights/departuredelays.csv\", \n",
    "                    header=True, inferSchema=True)\n",
    "\n",
    "# Register DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"flights\")\n",
    "\n",
    "# Now you can use SQL\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT origin, \n",
    "           AVG(delay) as avg_delay,\n",
    "           COUNT(*) as flight_count\n",
    "    FROM flights\n",
    "    WHERE delay > 100\n",
    "    GROUP BY origin\n",
    "    ORDER BY avg_delay DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "result.show()\n",
    "\n",
    "# Can also mix SQL and DataFrame operations\n",
    "spark.sql(\"SELECT * FROM flights WHERE delay > 200\").filter(col('distance') > 1000).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07f8de2b-a572-4d72-858c-97fe46740329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Part 8: Writing Results\n",
    "\n",
    "You've processed your data - now save it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc272673-794f-4956-ad72-8970a4be79d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write to Parquet (Recommended for big data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feb7d99c-0db1-499f-a69c-693ac2db0f50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\uD83D\uDCBE Writing Results\\n\")\n",
    "\n",
    "# Create a result to save\n",
    "result_to_save = df_large \\\n",
    "    .filter(col('delay') > 60) \\\n",
    "    .select('date', 'origin', 'destination', 'delay', 'distance')\n",
    "    \n",
    "print(f\"Preview of what would be saved: {result_to_save.count():,} rows\")\n",
    "result_to_save.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b84e3480-aaae-4014-b6fc-bccdd61cd4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write to CSV (For small results or Excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8124958-abc8-447b-a6dd-5b10aadeea37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "small_result = (\n",
    "    df_large\n",
    "    .filter(\n",
    "        col('delay') > 60\n",
    "    )\n",
    "    .groupBy('origin')\n",
    "    .agg(\n",
    "        avg('delay').alias('avg_delay'),\n",
    "        count('*').alias('flight_count')\n",
    "    )\n",
    "    .orderBy(\n",
    "        desc('avg_delay')\n",
    "    )\n",
    "    .limit(100)\n",
    ")\n",
    "\n",
    "display(small_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b123201-6efc-4375-bb04-76bf96c75c45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write Modes\n",
    "\n",
    "| Mode | What It Does | Use When |\n",
    "|------|-------------|----------|\n",
    "| `overwrite` | Replace existing data | Recreating results |\n",
    "| `append` | Add to existing data | Incremental updates |\n",
    "| `ignore` | Skip if exists | Don't want to overwrite |\n",
    "| `error` | Fail if exists | Safety - don't overwrite accidentally |\n",
    "\n",
    "**Most common:** `overwrite` for one-off analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8faf4af-d1ec-4eb6-8491-5cb7ee947f83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### File Formats Quick Reference\n",
    "\n",
    "| Format | Best For | Pros | Cons |\n",
    "|--------|----------|------|------|\n",
    "| **Parquet** | Big data storage | Fast, compressed, columnar | Not human-readable |\n",
    "| **CSV** | Small results, Excel | Readable, universal | Slow, no compression |\n",
    "| **JSON** | Nested data, APIs | Flexible schema | Larger files |\n",
    "| **Delta** | Production data lakes | ACID, time travel, fast | Needs Delta Lake setup |\n",
    "\n",
    "**Default choice: Use Parquet for everything unless you have a reason not to.**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "class_example",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}